{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('review_train.json',orient = 'records',lines = True)\n",
    "test = pd.read_json('review_test.json',orient = 'records',lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_language(text):\n",
    "    # First delete all common emoticons.\n",
    "    text = re.sub('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)','',text)\n",
    "    if re.sub('[\\W]+','',text) == '':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_lang_train = train[train.text.apply(not_language)].index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[not_lang_train,'lang_type'] = 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "for i in range(train.shape[0]):\n",
    "    if i in not_lang:\n",
    "        continue\n",
    "    else:\n",
    "        train.loc[i,'lang_type'] = detect(train.text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eng = train[train.lang_type == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    tagged = pos_tag(tokens)\n",
    "    for tag in tagged:\n",
    "        wordnet_pos = get_wordnet_pos(tag[1]) or wordnet.NOUN\n",
    "        lemmas.append(wnl.lemmatize(tag[0], pos=wordnet_pos))\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.pop(stop.index('but'))\n",
    "stop.pop(stop.index('not'))\n",
    "preposition = ['of','with','at','from','into','during',\n",
    "               'including','until','till','against','among',\n",
    "               'throughout','despite','towards','upon','concerning','to','in',\n",
    "               'for','on','by','about','like','through','over',\n",
    "               'before','between','after','since','without','under',\n",
    "               'within','along','following','across','behind',\n",
    "               'beyond','plus','except','but','up','out','around','down','off','above','near']\n",
    "for prep in preposition:\n",
    "    if prep in stop:\n",
    "        stop.pop(stop.index(prep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert n't to not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_abbreviation(text):\n",
    "    text = re.sub('n\\'t',' not',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adversatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "but = ['yet','however','nonetheless','whereas','nevertheless']\n",
    "although = ['although','though','notwithstanding','albeit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_but(text):\n",
    "    for x in but:\n",
    "        text = re.sub(x,'but',text)\n",
    "    return text\n",
    "def change_although(text):\n",
    "    for x in although:\n",
    "        text = re.sub(x,'although',text)\n",
    "    return text\n",
    "def change_adversatives(text):\n",
    "    text = change_but(text)\n",
    "    text = change_although(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # 取表情\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    # 去回车\n",
    "    text = re.sub('\\\\n',' ',text)\n",
    "    # not\n",
    "    text = no_abbreviation(text)\n",
    "    # 只保留字母\n",
    "    text = re.sub('[\\W]+',' ', text.lower())\n",
    "    # 统一转折词\n",
    "    text = change_adversatives(text)\n",
    "    # 词性还原\n",
    "    tokens = lemmatizer(text)\n",
    "    text = ''\n",
    "    for index, token in enumerate(tokens):\n",
    "        if token in stop:\n",
    "            tokens[index] = ''\n",
    "        else:\n",
    "            text = text + tokens[index] + ' '\n",
    "    return {'text':text,'emoticons':emoticons}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_pandas\n",
    "tqdm.pandas()\n",
    "dictionary_train = train_eng.text.progress_apply(preprocessing)\n",
    "dictionary_test = test.text.progress_apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_eng.loc[dictionary.index][\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = [dictionary_train[i]['text'] for i in train_eng.index]\n",
    "texts_test = [dictionary_test[i]['text'] for i in test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.append(texts_train,texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "\n",
    "new_texts = ['']\n",
    "for i in tqdm(range(len(texts))):\n",
    "    new_texts.append([spell(j) for j in texts[i].split(' ')])\n",
    "\n",
    "new_texts = new_texts[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['']\n",
    "for i in range(len(new_texts)):\n",
    "    result.append(' '.join(new_texts[i]))\n",
    "    \n",
    "new_texts = result[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams for phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_stream = [sent.split(' ') for sent in new_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(Phrases(sentence_stream, min_count=5, threshold=5)) #mincount越小识别出来的越少，threshold higher means fewer phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_with_phrase = bigram[sentence_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ['']\n",
    "for i in range(len(new_texts)):\n",
    "    result.append(' '.join(bigram[sentence_stream[i]]))\n",
    "    \n",
    "new_texts = result[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', min_df = 1, lowercase = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response =  tf.fit_transform(new_texts).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = response[:len(texts_train)]\n",
    "tfidf_test = response[len(texts_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr=LogisticRegression(multi_class='multinomial',solver='newton-cg’')\n",
    "lr.fit(tfidf_train,y)\n",
    "y_pred=lr.predict(tfidf_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
